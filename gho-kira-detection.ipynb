{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "import pathlib # To import dataset and working with paths\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ed0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset from the folder\n",
    "data_url = '/home/kayd/cs/projects/gho-kira-detection-ml/gho-kira-images'\n",
    "data_dir = pathlib.Path(data_url)\n",
    "\n",
    "image_count = len(list(data_dir.glob('*/*')))\n",
    "print('Image count: ', image_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae91d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data\n",
    "kiras = list(data_dir.glob('kira/*'))\n",
    "# print(kiras)\n",
    "PIL.Image.open(kiras[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-pocket",
   "metadata": {},
   "source": [
    "### To-Do\n",
    "- [] Research on what batch size I need to use\n",
    "- [] Research on what image size I need to put"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f2029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using keras utility\n",
    "\n",
    "# Parameters\n",
    "batch_size = 14\n",
    "img_height = 240\n",
    "img_width = 240\n",
    "\n",
    "# Use validation split - 80/20\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split = 0.2,\n",
    "    seed = 111,\n",
    "    subset = 'training', \n",
    "    image_size = (img_height, img_width),\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c350db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading validation set\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split = 0.2,\n",
    "    subset = 'validation',\n",
    "    seed = 111,\n",
    "    image_size = (img_height, img_width),\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c72052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print('Class names: ', class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1f8a6f",
   "metadata": {},
   "source": [
    "# Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a708e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (10, 10))\n",
    "\n",
    "for images, label in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype('uint8'))\n",
    "        plt.title(class_names[label[i]])\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1662ae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually iterate and retrieve batches of image\n",
    "for image_batch, labels_batch in train_ds:\n",
    "    print('Image batch: ', image_batch.shape)\n",
    "    print('Lables: ', label.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e064133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure dataset for performace\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a2c42",
   "metadata": {},
   "source": [
    "# Standardize the data\n",
    "\n",
    "Two ways to approach:\n",
    "1. Implement in the dataset\n",
    "2. Do it on the fly - we will use this approach\n",
    "\n",
    "Example if we did the first approach:\n",
    "```python\n",
    "normalization_layer = layers.Rescaling(1./255)\n",
    "\n",
    "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
    "image_batch, labels_batch = next(iter(normalized_ds))\n",
    "first_image = image_batch[0]\n",
    "# Notice the pixel values are now in `[0,1]`.\n",
    "print(np.min(first_image), np.max(first_image))\n",
    "# Output: 0.0 1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f7869c",
   "metadata": {},
   "source": [
    "# Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b5612",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(class_names)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b657e427",
   "metadata": {},
   "source": [
    "# Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd797738",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e59b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9e319c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(history))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01cc180",
   "metadata": {},
   "source": [
    "# Visualize Training Results\n",
    "\n",
    "The output here will tend towards overfitting as the sample size is low.\n",
    "So, we use data augmentation and other methods to train and get better results\n",
    "After we train the new model, we again visualize it.\n",
    "\n",
    "Finally, we will create a function where, the new images can be fed in and the model will predict.\n",
    "\n",
    "** To-Do: **\n",
    "- [] Research Visualising Data\n",
    "- [] Correct the error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bde0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d4a3c3",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "\n",
    "** To-Do **\n",
    "- [] Add more augmentation like changing color and stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6211868",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip('horizontal', \n",
    "                        input_shape=(img_height, img_width, 3)),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.1)\n",
    "])\n",
    "\n",
    "# data_augmentation = keras.Sequential([\n",
    "#     layers.RandomFlip('horizontal', input_shape=(img_height, img_width, 3)),\n",
    "#     layers.RandomRotation(0.1),\n",
    "#     layers.RandomZoom(0.1),\n",
    "#     layers.RandomTranslation(0.2, 0.2),\n",
    "#     layers.RandomHeight(0.1),\n",
    "#     layers.RandomWidth(0.1),\n",
    "#     layers.RandomContrast(0.5)\n",
    "# ])\n",
    "\n",
    "# data_augmentation = keras.Sequential([\n",
    "#     layers.RandomFlip('horizontal', \n",
    "#                         input_shape=(img_height, img_width, 3)),\n",
    "#     layers.RandomRotation(0.1),\n",
    "#     layers.RandomZoom(0.1),\n",
    "#     layers.RandomTranslation(0.2, 0.2),\n",
    "#     layers.RandomHeight(0.1),\n",
    "#     layers.RandomWidth(0.1),\n",
    "#     layers.RandomContrast(0.5)\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb91b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize few augmented images\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_ds.take(1):\n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(augmented_images[0].numpy().astype('uint8'))\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62238e3d",
   "metadata": {},
   "source": [
    "# Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcc9eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    data_augmentation,\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b26e3f",
   "metadata": {},
   "source": [
    "# Compile and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e413c0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04f39ae",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f394cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d8a8f8",
   "metadata": {},
   "source": [
    "# Visulaizing Training Results\n",
    "\n",
    "** To-Do ** \n",
    "- [] same as before. Add the visualization logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e7dfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b381e2ec",
   "metadata": {},
   "source": [
    "# Predict New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee5144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_data_url = '/home/kayd/cs/projects/gho-kira-detection-ml/gho-kira-images/test-tibetan-1.jpg'\n",
    "# new_data_path = pathlib.Path(new_data_url)\n",
    "\n",
    "# img = tf.keras.utils.load_img(\n",
    "#     new_data_path, target_size = (img_width, img_height)\n",
    "# )\n",
    "\n",
    "# img_array = tf.keras.utils.img_to_array(img)\n",
    "# img_array = tf.expand_dims(img_array, 0) # Creating a batch\n",
    "\n",
    "# predictions = model.predict(img_array)\n",
    "# score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "# print(\n",
    "#     \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "#     .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    "# )\n",
    "\n",
    "def predict_image(url):\n",
    "    new_data_url = url\n",
    "    new_data_path = pathlib.Path(new_data_url)\n",
    "\n",
    "    img = tf.keras.utils.load_img(\n",
    "        new_data_path, target_size = (img_width, img_height)\n",
    "    )\n",
    "\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.expand_dims(img_array, 0) # Creating a batch\n",
    "\n",
    "    predictions = model.predict(img_array)\n",
    "    score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "    print(\n",
    "        \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "        .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    "    )\n",
    "\n",
    "test_gho1 = '/home/kayd/cs/projects/gho-kira-detection-ml/gho-kira-images/test-gho-1.jpg'\n",
    "test_gho2 = '/home/kayd/cs/projects/gho-kira-detection-ml/gho-kira-images/test-gho-2.jpeg'\n",
    "test_gho3 = '/home/kayd/cs/projects/gho-kira-detection-ml/gho-kira-images/test-gho-3.jpg'\n",
    "test_gho4 = ''\n",
    "\n",
    "test_kira1 = '/home/kayd/cs/projects/gho-kira-detection-ml/gho-kira-images/test-kira-1.jpg'\n",
    "test_kira2 = '/home/kayd/cs/projects/gho-kira-detection-ml/gho-kira-images/test-kira-2.jpeg'\n",
    "test_kira3 = '/home/kayd/cs/projects/gho-kira-detection-ml/gho-kira-images/test-kira-3.jpg'\n",
    "test_kira4 = '/home/kayd/cs/projects/gho-kira-detection-ml/gho-kira-images/test-kira-4.jpg'\n",
    "\n",
    "predict_image(test_gho1)\n",
    "predict_image(test_gho2)\n",
    "predict_image(test_gho3)\n",
    "\n",
    "predict_image(test_kira1)\n",
    "predict_image(test_kira2)\n",
    "predict_image(test_kira3)\n",
    "predict_image(test_kira4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72759b48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
